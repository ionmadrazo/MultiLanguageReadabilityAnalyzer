\documentclass[11pt,twocolumn]{article}
\usepackage[T1]{fontenc}
\usepackage{titling}

\setlength{\droptitle}{-5em}   % This is your set screw

\usepackage{lingmacros}
\usepackage{tree-dvips}
\usepackage{datetime}
\usepackage{color}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{cite}
\graphicspath{ {images/} }
\title{Exploring social features for readability prediction}
\author{Ion Madrazo Azpiazu}
%\date{\today}
\date{\vspace{-5ex}}
%\date{}
\begin{document}


\maketitle
\section{Introduction}

Reading is an important skill in the academic environment, a skill that can be critical for students' educational opportunities and their careers \cite{robinson2000issues}. Giving students texts that suppose an increasing challenge to read during their learning process is essential. Even outside the educational environment, reading plays a important role. It is critical for people to fully comprehend the texts they read, specially when they face medical or legal issues. Understanding a legal or medical document properly, can lead the reader to make a better and more confident decision. However, studies\cite{medicalReadability1,medicalReadability2,medicalReadability3}  show that even medical documents that are supposed to be suited for average readers, tend to be too specialized and even well-educated adults have trouble understanding. Providing people with ways to ensure that the produced texts are simple enough for people with low reading abilities is imperative.



The readability or complexity of a text, and thus, the audience of it, can be assessed using a readability score. A readability score refers to the degree of ease with which a reader can understand a given text, a score which is usually determined by a readability formula. Historically, teachers have been the main stakeholders of readability formulas,using them for selecting new materials for their courses and curriculum design. However, lately, readability scores have been known to have more applicability than the ones in academic environments. Automatic text simplification\cite{textsimplification1,textsimplification2}, summarization for people with reading difficulties\cite{textsimplificationWithDisabilities1}, book recommendation \cite{pera2014automating}, literacy assessment\cite{literacy1}, or even legal\cite{legalreadability} and medical document complexity assessment\cite{medicalReadability1,medicalReadability2,medicalReadability3}  are only a few examples of applications that take advantage of the comprehension levels generated by existing readability scores.


Traditional formulas such as Flesh \cite{flesch1948new}, Dale-Chall \cite{chall1995readability}, and Gunning FOG \cite{albright1996readability} became very popular in the late 40's among the educators for manually determining text difficulty. Most of those formulas contained \textit{shallow features} which provided a simple way of determining a texts complexity. However, they lacked precision in some cases, such as the one claimed in \cite{davison1982failure} where nonsense text could be classified as simple to read, just because it contained short and frequently used words. This encouraged researchers to study and develop better and more complex methods of prediction \cite{franccois2012ai,aluisio2010readability}, that depended upon natural language processing and supervised machine learning techniques. These new formulas usually continued using the aforementioned shallow  features, but added more complex features based on contents of the text. 

Once the supervised learning framework became common in the field, most of the works have been focused on feature engineering. Some systems focused at lexical level \cite{feng2010comparison,collins2004language}, using term based metrics such as frequencies, Tf-IDf or mutual information. However, these formulas were shown not to be domain independent\cite{collins2004language}, due to the fact that the features learned where to close to the topics of the text. Other systems \cite{feng2009automatic,bonsall2015plain}, focused their work on the structure of the texts using syntactic features for capturing the complexity of the them. Semantic level features were also included in some readability assessment systems, such as the feature based on ambiguity the authors of \cite{aluisio2010readability} presented.





\section{Project proposal}

Readability assessment can be used in more than just plain text. Internet is evolving into a new social era and so are text resources too. Increasingly more resources contain social information such as hashtags, mentions or links, an information that is usually ignored by readability formulas. Therefore we would like to perform a preliminary research in order to see how the aforementioned information can be used for readability prediction, exploring with this, how social aspects of a texts can influence the readability of it. Furthermore, social media resources are known to be difficult to tackle by traditional natural language processing techniques, both because their lack of correctness and irregularity in terms of length. Therefore, we would also like to evaluate the performance traditional features have in social media texts.


\section{Proposed method}
The proposed system will be based on a supervised learning strategy. Given the ordinal and discrete nature of the class, we think that the task can fit both a regression and classification model. Therefore, both strategies will be tested during the development of the system. However, the main focus of the research will be on feature engineering, studying features that make use of social aspects in the texts i.e. mentions, hashtags, links or emoticons.






\subsection{Text processing}

Different text processing methods have been identified for the development of the system. Freeling NLP \cite{padro12,padro10b} is a multilingual natural language processing (NLP) toolkit that supports 11 different languages. This tool solves common NLP tasks such as, tokenization, sentence detection, part of speech tagging or dependency parsing. WordNet is a lexical database that takes advantage of semantic relations between terms to build a graph that is very convenient for semantic analysis tasks. Latent semantic analysis is also a commonly used strategy  for semantic analysis, which takes advantage of concurrences among terms for determining similarities between them. All those tools and others that we will discover in the process of development, will form the text processing step of the system.
 




\subsection{Dataset}
For development and evaluation purposes a dataset will be needed. The ideal dataset would be one containing social media documents with a level label attached to them. However, to the best of our knowledge, such dataset does not currently exist. For this reason, an approximation to the aforementioned ideal dataset will be created for the project. For doing so, we will analyze a twitter sample for the year 2015 consisting of 2\% of the tweets created in that period. A graph will be generated using the mentions among the tweets and communities will be detected on it, using a yet to be determined algorithm. Those communities will be manually analyzed and tagged according to the readability level of the tweets the users inside them create. The tweets will be assigned the readability level of the community, generating with this a dataset of leveled tweets.

{\footnotesize
\bibliography{bibliography}{}

\bibliographystyle{abbrv}
\end{document}